{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596303490743",
   "display_name": "Python 3.7.7 64-bit ('lang': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import scipy\n",
    "import cv2\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from google_images_download import google_images_download\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Synset('pelecaniform_seabird.n.01'),\n Synset('seabird.n.01'),\n Synset('aquatic_bird.n.01'),\n Synset('bird.n.01'),\n Synset('vertebrate.n.01'),\n Synset('chordate.n.01'),\n Synset('animal.n.01'),\n Synset('organism.n.01'),\n Synset('living_thing.n.01'),\n Synset('whole.n.02'),\n Synset('object.n.01'),\n Synset('physical_entity.n.01'),\n Synset('entity.n.01')]"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# example of one synset\n",
    "pelican = wn.synset('pelican.n.01')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(pelican.closure(hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "noun: good\nnoun: good, goodness\nnoun: good, goodness\nnoun: commodity, trade_good, good\nadj: good\nadj (s): full, good\nadj: good\nadj (s): estimable, good, honorable, respectable\nadj (s): beneficial, good\nadj (s): good\nadj (s): good, just, upright\nadj (s): adept, expert, good, practiced, proficient, skillful, skilful\nadj (s): good\nadj (s): dear, good, near\nadj (s): dependable, good, safe, secure\nadj (s): good, right, ripe\nadj (s): good, well\nadj (s): effective, good, in_effect, in_force\nadj (s): good\nadj (s): good, serious\nadj (s): good, sound\nadj (s): good, salutary\nadj (s): good, honest\nadj (s): good, undecomposed, unspoiled, unspoilt\nadj (s): good\nadv: well, good\nadv: thoroughly, soundly, good\n"
    }
   ],
   "source": [
    "# example of categorization with one synset\n",
    "poses = {'n':'noun', 'v':'verb', 's':'adj (s)', 'a': 'adj', 'r':'adv'}\n",
    "for synset in wn.synsets('good'):\n",
    "    print('{}: {}'.format(poses[synset.pos()],\n",
    "    ', '.join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of nouns: 8\n"
    }
   ],
   "source": [
    "nouns = set()\n",
    "for synset in list(wn.all_synsets('n'))[:10]:\n",
    "\n",
    "    n = synset.name().split('.')[0]\n",
    "\n",
    "    if len(n) > 2 and n.isalpha():\n",
    "        nouns.add(n)\n",
    "        \n",
    "print('Number of nouns:', len(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nItem no.: 1 --> Item name = congener\nEvaluating...\nStarting Download...\n\nErrors: 0\n\n({'congener': []}, 0)\n\nItem no.: 1 --> Item name = entity\nEvaluating...\nStarting Download...\n\nErrors: 0\n\n({'entity': []}, 0)\n\nItem no.: 1 --> Item name = whole\nEvaluating...\nStarting Download...\n\nErrors: 0\n\n({'whole': []}, 0)\n\nItem no.: 1 --> Item name = organism\nEvaluating...\nStarting Download...\n\nErrors: 0\n\n({'organism': []}, 0)\n\nItem no.: 1 --> Item name = abstraction\nEvaluating...\nStarting Download...\n\nErrors: 0\n\n({'abstraction': []}, 0)\n\nItem no.: 1 --> Item name = benthos\nEvaluating...\nStarting Download...\n\nErrors: 0\n\n({'benthos': []}, 0)\n\nItem no.: 1 --> Item name = object\nEvaluating...\nStarting Download...\n\nErrors: 0\n\n({'object': []}, 0)\n\nItem no.: 1 --> Item name = thing\nEvaluating...\nStarting Download...\n\nErrors: 0\n\n({'thing': []}, 0)\n"
    }
   ],
   "source": [
    "resnet = ResNet50(weights='imagenet', \n",
    "                  include_top=False, \n",
    "                  pooling='avg')\n",
    "\n",
    "embeddings = KeyedVectors(2048)\n",
    "\n",
    "for word in nouns:\n",
    "    response = google_images_download.googleimagesdownload()\n",
    "    path = response.download({'keywords': word, 'limit': 1, 'print_urls':True})\n",
    "    print(path)\n",
    "    #img = cv2.imread(path)\n",
    "    #img = scipy.misc.imresize(img, 224.0 / img.shape[0])\n",
    "    #img = img.reshape((1,) + img.shape)\n",
    "    #embeddings[word] = resnet.predict(img)[0]\n",
    "    \n",
    "#print('Vocabulary size:', len(embeddings.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, vectors = zip(*map(lambda v: (v, embeddings[v]), embeddings.vocab))\n",
    "print('# labels:', len(labels), '# vectors:', len(vectors)"
   ]
  }
 ]
}